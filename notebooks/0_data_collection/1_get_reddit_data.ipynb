{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Reddit Data\n",
    "\n",
    "This notebook will grab all the reddit data used. In order to run this notebook, the following need to be set up: \n",
    "1. **[PRAW API](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html)** created and saved within this directory in the file called \"credentials.json\". \n",
    "    - This will allow access to the Reddit data\n",
    "2. **[Google Drive API](https://developers.google.com/drive/api/quickstart/go)** created and saved within this directory in the file called \"client_secrets.json\"\n",
    "    - This will allow access to Google Drive to save the data in\n",
    "    - Also create the Google Drive Folder ID Credentials file (\"google_drive_credentials.json\") that will contain the folder id of the folder in google drive you'd like to save it to \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# set this on the path so that we can reference the commong data locations\n",
    "sys.path.append(\"../../scripts/\")\n",
    "from data_collection import load_credentials, combine_subreddits, posts_to_comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "import sys \n",
    "import json #needed to translate JSON data\n",
    "import requests #needed to perform HTTP GET and POST requests\n",
    "import pandas as pd\n",
    "import pprint # allows us to print more readable JSON data\n",
    "from datetime import datetime \n",
    "import time \n",
    "import io\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100) # Need this otherwise text columns will truncate!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged into Reddit successfully\n"
     ]
    }
   ],
   "source": [
    "# Load in the credentials\n",
    "reddit = load_credentials('credentials.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done pulling FirstTimeHomeBuyer subreddit for search term Rocket!\n",
      "Done pulling RealEstate subreddit for search term Rocket!\n",
      "Done pulling FirstTimeHomeBuyer subreddit for search term Fargo!\n",
      "Done pulling RealEstate subreddit for search term Fargo!\n",
      "====DONE!====\n",
      "(20, 10)\n",
      "--- Grabbed the data in: 13.46 minutes ---\n",
      "--- Grabbed comments from the posts in: 0.105 seconds ---\n",
      "CPU times: user 612 ms, sys: 34.6 ms, total: 646 ms\n",
      "Wall time: 22.1 s\n"
     ]
    }
   ],
   "source": [
    "n_posts = 5 # Start small!\n",
    "sub_reddits = ['FirstTimeHomeBuyer', 'RealEstate']#, 'loanoriginators', \n",
    "                # 'homeowners', 'Mortgages', 'personalfinance']\n",
    "search_terms = [\"Rocket\", \"Fargo\"]\n",
    "\n",
    "start_collect_time = time.time()\n",
    "\n",
    "# grab the posts\n",
    "reddit_data = combine_subreddits(reddit, n_posts, sub_reddits, search_terms)\n",
    "end_collect_time = time.time()\n",
    "time_elapsed = round((end_collect_time - start_time)/60.0, 2)\n",
    "print(\"--- Grabbed the data in: %s minutes ---\" % (time_elapsed))\n",
    "\n",
    "# grab comments from posts\n",
    "comments_combined_df = posts_to_comments(reddit_data)\n",
    "time_elapsed = round(time.time() - end_collect_time, 3)\n",
    "print(\"--- Grabbed comments from the posts in: %s seconds ---\" % (time_elapsed))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can only save the data location locally temporarily and will need to delete \n",
    "# so that we can comply to Reddit policy \n",
    "comments_combined_df.to_csv('../../data/comments.csv', index = False)\n",
    "reddit_data.to_csv('../../data/posts.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=209917166075-saupbq0ls0he9jdlpjrtscaio8kf1m7p.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "\n",
      "Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "# Authenticate with Google\n",
    "gauth = GoogleAuth()\n",
    "gauth.LocalWebserverAuth()  # Opens a browser for authentication\n",
    "\n",
    "# Create GoogleDrive instance\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the Folder Id of the google drive where the data will be saved\n",
    "with open(\"google_drive_credentials.json\", 'r') as file:\n",
    "    google_drive_credentials = json.load(file)\n",
    "folder_id = google_drive_credentials[\"folder_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_google_drive(dataframe, filename): \n",
    "    csv_buffer = io.StringIO()\n",
    "    dataframe.to_csv(csv_buffer, index=False)\n",
    "    csv_buffer.seek(0)\n",
    "\n",
    "    file = drive.CreateFile({'title': filename, 'parents': [{'id': folder_id}]})\n",
    "    file.SetContentString(csv_buffer.getvalue())  # Set content from memory buffer\n",
    "    file.Upload()\n",
    "    print(f\"File '{filename}' uploaded successfully to folder {folder_id}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'reddit_data.csv' uploaded successfully to folder 1kJ6TrI9MVT5mfnnYvS-OpRMJFVbIQ6Tl!\n"
     ]
    }
   ],
   "source": [
    "save_data_google_drive(reddit_data, \"reddit_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'reddit_comments_data.csv' uploaded successfully to folder 1kJ6TrI9MVT5mfnnYvS-OpRMJFVbIQ6Tl!\n"
     ]
    }
   ],
   "source": [
    "save_data_google_drive(comments_combined_df, \"reddit_comments_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Search for the file in the folder\n",
    "# file_list = drive.ListFile({'q': f\"'{folder_id}' in parents and trashed=false\"}).GetList()\n",
    "\n",
    "# # Find the specific file by name\n",
    "# filename = \"posts.csv\"\n",
    "# file_id = None\n",
    "# for file in file_list:\n",
    "#     if file['title'] == filename:\n",
    "#         file_id = file['id']\n",
    "#         break\n",
    "\n",
    "# if file_id:\n",
    "#     # Download file content into memory\n",
    "#     file = drive.CreateFile({'id': file_id})\n",
    "#     file_content = io.StringIO(file.GetContentString())\n",
    "\n",
    "#     # Load into a Pandas DataFrame\n",
    "#     df = pd.read_csv(file_content)\n",
    "\n",
    "#     print(f\"Successfully loaded '{filename}' into a DataFrame!\")\n",
    "#     print(df.head())  # Print first few rows\n",
    "# else:\n",
    "#     print(f\"File '{filename}' not found in folder {folder_id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
