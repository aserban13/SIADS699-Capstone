{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MADS Capstone Project: Text Analysis of Reddit Data for Mortgage Companies\n",
    "\n",
    "#### Authors: \n",
    "###### Andreea Serban and Chris McAllister\n",
    "----\n",
    "##### The goal of this notebook is to access the Reddit API to see how people feel about different mortgage companies. The output will two dataframes:\n",
    "1) One for all the posts on Reddit\n",
    "2) One for all the comments associated with the posts\n",
    "3) Lastly, we save it to our data folder.\n",
    "----\n",
    "##### Once we  have the data, we'll conduct to understand the following questions:\n",
    "1) Sentiment Trends Overtime\n",
    "2) Comparison of Sentiment to other mortgage companies\n",
    "3) Identify Similar Brands based off what Redditors are saying (Coca-Cola, Xfinity, etc).\n",
    "4) Identify other intersts of top posters\n",
    "\n",
    "Documentation for Reedit API: https://praw.readthedocs.io/en/stable/getting_started/quick_start.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "import sys \n",
    "from IPython.display import clear_output\n",
    "import json #needed to translate JSON data\n",
    "import requests #needed to perform HTTP GET and POST requests\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None) # Need this otherwise text columns will truncate!\n",
    "import pprint #allows us to print more readable JSON data\n",
    "from datetime import datetime \n",
    "import time \n",
    "\n",
    "#We will use the PRAW library to access Reddit data, version 7.7.1\n",
    "try:\n",
    "    import praw\n",
    "except:\n",
    "    !pip3 install praw==7.8.1 #this will install the version of PRAW that we need if PRAW does not already exist\n",
    "    import praw\n",
    "\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "praw version: 7.7.1\n"
     ]
    }
   ],
   "source": [
    "print(\"praw version:\",praw.__version__) #it can be helpful to confirm the version we're using for our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Grab these credentials from: https://www.reddit.com/prefs/apps\n",
    "# Function to load credentials from a JSON file\n",
    "def load_credentials(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        credentials = json.load(file)\n",
    "    return credentials\n",
    "\n",
    "# Path to the credentials file\n",
    "file_path = 'credentials.json'\n",
    "\n",
    "# Load the credentials\n",
    "credentials = load_credentials(file_path)\n",
    "\n",
    "# Assigning the credentials to variables\n",
    "REDDIT_USERNAME = credentials['REDDIT_USERNAME']\n",
    "REDDIT_PASSWORD = credentials['REDDIT_PASSWORD']\n",
    "APP_ID = credentials['APP_ID']\n",
    "APP_SECRET = credentials['APP_SECRET']\n",
    "APP_NAME = credentials['APP_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.7.1 of praw is outdated. Version 7.8.1 was released Friday October 25, 2024.\n"
     ]
    }
   ],
   "source": [
    "#Generate your reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=APP_ID,\n",
    "    client_secret=APP_SECRET,\n",
    "    user_agent=APP_NAME,\n",
    "    username=REDDIT_USERNAME, \n",
    "    password=REDDIT_PASSWORD,\n",
    "    check_for_async=False # This additional parameter supresses a warning about \"Asynchronous PRAW\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_df(subredit_topic=\"FirstTimeHomeBuyer\", n_posts=5, search_query='Rocket', include_edit = False): \n",
    "    \n",
    "    \"\"\"\n",
    "    This function accesses a subreddit page and searches it for a given phrase. It then accesses the top-n posts, and stores the post as a record in a dataframe.\n",
    "\n",
    "    args:\n",
    "    subredit_topic (str): The name of the subreddit we want to access (ie r/subredit_topic).\n",
    "    n_posts (int): The number of posts we want get from that subreddit\n",
    "    search_query (str): The phrase we want to use to extract posts.\n",
    "    include_edit (boolean): FALSE will not filter the post. TRUE will remove all text after the phrase \"EDIT: \"\n",
    "    \n",
    "    returns Pandas DataFrame with the text of \n",
    "    \"\"\"\n",
    "    \n",
    "    subreddit = reddit.subreddit(subredit_topic)\n",
    "    submissions = subreddit.search(search_query, limit=n_posts)\n",
    "\n",
    "    # submissions = subreddit.hot(limit=n_posts)\n",
    "    data = {\n",
    "        'submission_id': [],\n",
    "        'subredit_topic': [],\n",
    "        'search_query': [],\n",
    "        'title': [],\n",
    "        'text': [],\n",
    "        'score': [],\n",
    "        'num_comments': [],\n",
    "        'username': [],\n",
    "        'created_at': [],\n",
    "        # Added \n",
    "        'comment_dict': []\n",
    "    }\n",
    "    for submission in submissions:\n",
    "        data['submission_id'].append(submission.id)\n",
    "        data['subredit_topic'].append(subredit_topic)\n",
    "        data['search_query'].append(search_query)\n",
    "        data['title'].append(submission.title)\n",
    "        data['text'].append(submission.selftext)\n",
    "        data['score'].append(submission.score)\n",
    "        data['num_comments'].append(submission.num_comments)\n",
    "        data['username'].append(submission.author.name if submission.author else 'Deleted')\n",
    "        data['created_at'].append(submission.created_utc)\n",
    "\n",
    "        # added to get a dict of comments saved as a column\n",
    "        comment_dict = {}\n",
    "        for i, comment in enumerate(submission.comments):\n",
    "        \n",
    "            try:\n",
    "                comment_i = comment.body\n",
    "                comment_dict[i] = comment_i\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        data['comment_dict'].append(comment_dict)\n",
    "\n",
    "    # Create the dataframe from our dictionary\n",
    "    submission_df = pd.DataFrame(data)\n",
    "    submission_df['created_at'] = pd.to_datetime(submission_df['created_at'], unit='s') \n",
    "    \n",
    "    # Remove \\n\\n using str.replace()\n",
    "    submission_df['text'] = submission_df['text'].str.replace(r'\\n\\n', '', regex=True)\n",
    "\n",
    "    # Remove part of the text that gets edited if include_edit == True\n",
    "    if include_edit == True:\n",
    "        submission_df['text'] = submission_df['text'].str.replace(r'(?i)edit: .*', '', regex=True)\n",
    "\n",
    "    # Specify search term at end of dataset creation\n",
    "    submission_df['search_query'] = search_query\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_posts = 5 #we recommend lowering these numbers while you build your code.  Start with 1-2!\n",
    "\n",
    "submission_df = create_submission_df(\"FirstTimeHomeBuyer\", 5, \"Rocket\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to hit mulitple subreddit and companies\n",
    "def combine_subreddits(n_posts, sub_reddits, search_terms):\n",
    "\n",
    "    \"\"\"\n",
    "    This function combines data from multiple sub-reddits and search terms to create one large dataframe for analysis. \n",
    "\n",
    "    args:\n",
    "    n_posts (int): The number of posts that we want to access for each subreddit / search combination\n",
    "    sub_reddits (list): A list of sub reddits we want to access\n",
    "    search_terms (list): A list of terms we want to search on each subreddit (usually the name of a company). \n",
    "\n",
    "    returns:\n",
    "    a pandas DataFrame with all the data about the posts in one big dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    dfs = []\n",
    "    for company in search_terms:\n",
    "    \n",
    "        for sr in sub_reddits:\n",
    "        \n",
    "            sr_data = create_submission_df(subredit_topic = sr, n_posts = n_posts, search_query = company)\n",
    "            sr_data = dfs.append(sr_data)\n",
    "        \n",
    "            print(\"Done pulling \" + sr + \" subreddit for search term \" + company + \"!\")\n",
    "    \n",
    "    # Union all dataframes together\n",
    "    df_final = pd.concat(dfs)\n",
    "    print('====DONE!====')\n",
    "    print(df_final.shape)\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done pulling FirstTimeHomeBuyer subreddit for search term Rocket!\n",
      "Done pulling RealEstate subreddit for search term Rocket!\n",
      "Done pulling loanoriginators subreddit for search term Rocket!\n",
      "Done pulling homeowners subreddit for search term Rocket!\n",
      "Done pulling Mortgages subreddit for search term Rocket!\n",
      "Done pulling personalfinance subreddit for search term Rocket!\n",
      "Done pulling FirstTimeHomeBuyer subreddit for search term Fargo!\n",
      "Done pulling RealEstate subreddit for search term Fargo!\n",
      "Done pulling loanoriginators subreddit for search term Fargo!\n",
      "Done pulling homeowners subreddit for search term Fargo!\n",
      "Done pulling Mortgages subreddit for search term Fargo!\n",
      "Done pulling personalfinance subreddit for search term Fargo!\n",
      "====DONE!====\n",
      "(60, 10)\n"
     ]
    }
   ],
   "source": [
    "n_posts = 5 # Start small!\n",
    "sub_reddits = ['FirstTimeHomeBuyer', 'RealEstate', 'loanoriginators', 'homeowners', 'Mortgages', 'personalfinance']\n",
    "search_terms = [\"Rocket\", \"Fargo\"]\n",
    "\n",
    "\n",
    "reddit_data = combine_subreddits(n_posts, sub_reddits, search_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract comments as a df\n",
    "def posts_to_comments(data):\n",
    "\n",
    "    \"\"\"\n",
    "    The goal of this data is to extract the comments from a post. In our last function combine_subreddits() we created a columns\n",
    "    saved as a dictioanry of comments.\n",
    "\n",
    "    This function takes that dataset as input, and converts it into a df comprised of commments. We will have 1 row for each commment in this new df.\n",
    "\n",
    "    args:\n",
    "    data (Pandas DataFrame): A dataset of reddit posts. The output of combine_subreddits()\n",
    "\n",
    "    returns:\n",
    "    a Pandas DataFrame of comments. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dfs_list = []\n",
    "    for index, row in data.iterrows():\n",
    "        df_i = pd.DataFrame([row['comment_dict']]).T.reset_index()\n",
    "        df_i = df_i.rename({'index':'comment_index', 0: 'comment_text'}, axis = 1)\n",
    "        df_i['post_id'] = row['submission_id']\n",
    "        df_i['sub_reddit'] = row['subredit_topic']\n",
    "        df_i['post_time'] = row['created_at']\n",
    "        df_i['post_comment_count'] = row['num_comments']\n",
    "        df_i['poster_username'] = row['username']\n",
    "    \n",
    "        dfs_list.append(df_i)\n",
    "    \n",
    "    comment_df = pd.concat(dfs_list)\n",
    "    \n",
    "    # Remove \\n\\n using str.replace()\n",
    "    comment_df['comment_text'] = comment_df['comment_text'].str.replace(r'\\n\\n', '', regex=True)\n",
    "    comment_df['comment_text'] = comment_df['comment_text'].str.replace(r'\\*. \\n\\*', '', regex=True)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    return comment_df\n",
    "\n",
    "comments_df = posts_to_comments(reddit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3738, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit_data.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments_df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_index</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>post_id</th>\n",
       "      <th>sub_reddit</th>\n",
       "      <th>post_time</th>\n",
       "      <th>post_comment_count</th>\n",
       "      <th>poster_username</th>\n",
       "      <th>subredit_topic</th>\n",
       "      <th>search_query</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>16</td>\n",
       "      <td>FYI to everyone who is as entertained at this post as I am, OP posted in this sub a few months ago asking about Rocket, and people in this sub warned her that they would take on a bunch of fees right before closing. And now she is big mad that they did it, after being warned: https://www.reddit.com/r/FirstTimeHomeBuyer/comments/11mjqqx/what_mortgage_lender_would_you_recommend/jbjw1lg/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=ioscss&amp;utm_content=1&amp;utm_term=1&amp;context=3</td>\n",
       "      <td>1541el1</td>\n",
       "      <td>FirstTimeHomeBuyer</td>\n",
       "      <td>2023-07-19 17:31:01</td>\n",
       "      <td>471</td>\n",
       "      <td>justmeAlonekitty</td>\n",
       "      <td>FirstTimeHomeBuyer</td>\n",
       "      <td>Rocket</td>\n",
       "      <td>WARNING- do NOT work with rocket mortgage!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     comment_index  \\\n",
       "122             16   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   comment_text  \\\n",
       "122  FYI to everyone who is as entertained at this post as I am, OP posted in this sub a few months ago asking about Rocket, and people in this sub warned her that they would take on a bunch of fees right before closing. And now she is big mad that they did it, after being warned: https://www.reddit.com/r/FirstTimeHomeBuyer/comments/11mjqqx/what_mortgage_lender_would_you_recommend/jbjw1lg/?utm_source=share&utm_medium=ios_app&utm_name=ioscss&utm_content=1&utm_term=1&context=3   \n",
       "\n",
       "     post_id          sub_reddit           post_time  post_comment_count  \\\n",
       "122  1541el1  FirstTimeHomeBuyer 2023-07-19 17:31:01                 471   \n",
       "\n",
       "      poster_username      subredit_topic search_query  \\\n",
       "122  justmeAlonekitty  FirstTimeHomeBuyer       Rocket   \n",
       "\n",
       "                                            title  \n",
       "122  WARNING- do NOT work with rocket mortgage!!!  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join comments to original posts datset to get some more info \n",
    "comments_combined = pd.merge(left = comments_df,\n",
    "                             right = reddit_data,\n",
    "                             how = 'left',\n",
    "                             left_on = 'post_id',\n",
    "                             right_on = 'submission_id'\n",
    "                            )\n",
    "\n",
    "comments_combined = comments_combined[['comment_index', 'comment_text', 'post_id', 'sub_reddit', 'post_time', 'post_comment_count',\n",
    "                                       'poster_username', 'subredit_topic', 'search_query', 'title']]\n",
    "\n",
    "\n",
    "comments_combined.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to our data folder\n",
    "comments_combined.to_csv('../../data/comments.csv', index = False)\n",
    "reddit_data.to_csv('../../data/posts.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
